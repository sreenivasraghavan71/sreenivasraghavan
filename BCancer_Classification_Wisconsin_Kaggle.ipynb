{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.grid_search import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BCancer_Data=pd.read_csv(\"BCacncerWisconsin.csv\")\n",
    "BCancer_Data.dropna(axis=0, how='all', thresh=None, subset=None, inplace=True)\n",
    "BCancer_Data.drop('Unnamed: 32', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0  842302         M        17.99         10.38           122.8     1001.0   \n",
       "1  842517         M        20.57         17.77           132.9     1326.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33            184.6   \n",
       "1           ...                    24.99          23.41            158.8   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BCancer_Data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      "id                         569 non-null int64\n",
      "diagnosis                  569 non-null object\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 146.7+ KB\n"
     ]
    }
   ],
   "source": [
    "BCancer_Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
       "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
       "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
       "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
       "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean           ...             radius_worst  texture_worst  \\\n",
       "count     569.000000           ...               569.000000     569.000000   \n",
       "mean        0.181162           ...                16.269190      25.677223   \n",
       "std         0.027414           ...                 4.833242       6.146258   \n",
       "min         0.106000           ...                 7.930000      12.020000   \n",
       "25%         0.161900           ...                13.010000      21.080000   \n",
       "50%         0.179200           ...                14.970000      25.410000   \n",
       "75%         0.195700           ...                18.790000      29.720000   \n",
       "max         0.304000           ...                36.040000      49.540000   \n",
       "\n",
       "       perimeter_worst   area_worst  smoothness_worst  compactness_worst  \\\n",
       "count       569.000000   569.000000        569.000000         569.000000   \n",
       "mean        107.261213   880.583128          0.132369           0.254265   \n",
       "std          33.602542   569.356993          0.022832           0.157336   \n",
       "min          50.410000   185.200000          0.071170           0.027290   \n",
       "25%          84.110000   515.300000          0.116600           0.147200   \n",
       "50%          97.660000   686.500000          0.131300           0.211900   \n",
       "75%         125.400000  1084.000000          0.146000           0.339100   \n",
       "max         251.200000  4254.000000          0.222600           1.058000   \n",
       "\n",
       "       concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "count       569.000000            569.000000      569.000000   \n",
       "mean          0.272188              0.114606        0.290076   \n",
       "std           0.208624              0.065732        0.061867   \n",
       "min           0.000000              0.000000        0.156500   \n",
       "25%           0.114500              0.064930        0.250400   \n",
       "50%           0.226700              0.099930        0.282200   \n",
       "75%           0.382900              0.161400        0.317900   \n",
       "max           1.252000              0.291000        0.663800   \n",
       "\n",
       "       fractal_dimension_worst  \n",
       "count               569.000000  \n",
       "mean                  0.083946  \n",
       "std                   0.018061  \n",
       "min                   0.055040  \n",
       "25%                   0.071460  \n",
       "50%                   0.080040  \n",
       "75%                   0.092080  \n",
       "max                   0.207500  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BCancer_Data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      "id                         569 non-null int64\n",
      "diagnosis                  569 non-null int64\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "dtypes: float64(30), int64(2)\n",
      "memory usage: 146.7 KB\n"
     ]
    }
   ],
   "source": [
    "w={}\n",
    "w[\"M\"]=0\n",
    "w[\"B\"]=1\n",
    "BCancer_Data[\"diagnosis\"]=BCancer_Data[\"diagnosis\"].map(lambda x:w[x])\n",
    "BCancer_Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.097064</td>\n",
       "      <td>-2.073335</td>\n",
       "      <td>1.269934</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>1.568466</td>\n",
       "      <td>3.283515</td>\n",
       "      <td>2.652874</td>\n",
       "      <td>2.532475</td>\n",
       "      <td>2.217515</td>\n",
       "      <td>2.255747</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886690</td>\n",
       "      <td>-1.359293</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>2.001237</td>\n",
       "      <td>1.307686</td>\n",
       "      <td>2.616665</td>\n",
       "      <td>2.109526</td>\n",
       "      <td>2.296076</td>\n",
       "      <td>2.750622</td>\n",
       "      <td>1.937015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.829821</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>1.685955</td>\n",
       "      <td>1.908708</td>\n",
       "      <td>-0.826962</td>\n",
       "      <td>-0.487072</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>-0.868652</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805927</td>\n",
       "      <td>-0.369203</td>\n",
       "      <td>1.535126</td>\n",
       "      <td>1.890489</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>-0.430444</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>1.087084</td>\n",
       "      <td>-0.243890</td>\n",
       "      <td>0.281190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.579888</td>\n",
       "      <td>0.456187</td>\n",
       "      <td>1.566503</td>\n",
       "      <td>1.558884</td>\n",
       "      <td>0.942210</td>\n",
       "      <td>1.052926</td>\n",
       "      <td>1.363478</td>\n",
       "      <td>2.037231</td>\n",
       "      <td>0.939685</td>\n",
       "      <td>-0.398008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511870</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>1.347475</td>\n",
       "      <td>1.456285</td>\n",
       "      <td>0.527407</td>\n",
       "      <td>1.082932</td>\n",
       "      <td>0.854974</td>\n",
       "      <td>1.955000</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.201391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.768909</td>\n",
       "      <td>0.253732</td>\n",
       "      <td>-0.592687</td>\n",
       "      <td>-0.764464</td>\n",
       "      <td>3.283553</td>\n",
       "      <td>3.402909</td>\n",
       "      <td>1.915897</td>\n",
       "      <td>1.451707</td>\n",
       "      <td>2.867383</td>\n",
       "      <td>4.910919</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>0.133984</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>-0.550021</td>\n",
       "      <td>3.394275</td>\n",
       "      <td>3.893397</td>\n",
       "      <td>1.989588</td>\n",
       "      <td>2.175786</td>\n",
       "      <td>6.046041</td>\n",
       "      <td>4.935010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.750297</td>\n",
       "      <td>-1.151816</td>\n",
       "      <td>1.776573</td>\n",
       "      <td>1.826229</td>\n",
       "      <td>0.280372</td>\n",
       "      <td>0.539340</td>\n",
       "      <td>1.371011</td>\n",
       "      <td>1.428493</td>\n",
       "      <td>-0.009560</td>\n",
       "      <td>-0.562450</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298575</td>\n",
       "      <td>-1.466770</td>\n",
       "      <td>1.338539</td>\n",
       "      <td>1.220724</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>-0.313395</td>\n",
       "      <td>0.613179</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>-0.868353</td>\n",
       "      <td>-0.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.476375</td>\n",
       "      <td>-0.835335</td>\n",
       "      <td>-0.387148</td>\n",
       "      <td>-0.505650</td>\n",
       "      <td>2.237421</td>\n",
       "      <td>1.244335</td>\n",
       "      <td>0.866302</td>\n",
       "      <td>0.824656</td>\n",
       "      <td>1.005402</td>\n",
       "      <td>1.890005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165498</td>\n",
       "      <td>-0.313836</td>\n",
       "      <td>-0.115009</td>\n",
       "      <td>-0.244320</td>\n",
       "      <td>2.048513</td>\n",
       "      <td>1.721616</td>\n",
       "      <td>1.263243</td>\n",
       "      <td>0.905888</td>\n",
       "      <td>1.754069</td>\n",
       "      <td>2.241802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.170908</td>\n",
       "      <td>0.160649</td>\n",
       "      <td>1.138125</td>\n",
       "      <td>1.095295</td>\n",
       "      <td>-0.123136</td>\n",
       "      <td>0.088295</td>\n",
       "      <td>0.300072</td>\n",
       "      <td>0.646935</td>\n",
       "      <td>-0.064325</td>\n",
       "      <td>-0.762332</td>\n",
       "      <td>...</td>\n",
       "      <td>1.368983</td>\n",
       "      <td>0.322883</td>\n",
       "      <td>1.368325</td>\n",
       "      <td>1.275220</td>\n",
       "      <td>0.518640</td>\n",
       "      <td>0.021215</td>\n",
       "      <td>0.509552</td>\n",
       "      <td>1.196716</td>\n",
       "      <td>0.262476</td>\n",
       "      <td>-0.014730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.118517</td>\n",
       "      <td>0.358450</td>\n",
       "      <td>-0.072867</td>\n",
       "      <td>-0.218965</td>\n",
       "      <td>1.604049</td>\n",
       "      <td>1.140102</td>\n",
       "      <td>0.061026</td>\n",
       "      <td>0.281950</td>\n",
       "      <td>1.403355</td>\n",
       "      <td>1.660353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163763</td>\n",
       "      <td>0.401048</td>\n",
       "      <td>0.099449</td>\n",
       "      <td>0.028859</td>\n",
       "      <td>1.447961</td>\n",
       "      <td>0.724786</td>\n",
       "      <td>-0.021054</td>\n",
       "      <td>0.624196</td>\n",
       "      <td>0.477640</td>\n",
       "      <td>1.726435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.320167</td>\n",
       "      <td>0.588830</td>\n",
       "      <td>-0.184080</td>\n",
       "      <td>-0.384207</td>\n",
       "      <td>2.201839</td>\n",
       "      <td>1.684010</td>\n",
       "      <td>1.219096</td>\n",
       "      <td>1.150692</td>\n",
       "      <td>1.965600</td>\n",
       "      <td>1.572462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161357</td>\n",
       "      <td>0.822813</td>\n",
       "      <td>-0.031609</td>\n",
       "      <td>-0.248363</td>\n",
       "      <td>1.662757</td>\n",
       "      <td>1.818310</td>\n",
       "      <td>1.280035</td>\n",
       "      <td>1.391616</td>\n",
       "      <td>2.389857</td>\n",
       "      <td>1.288650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.473535</td>\n",
       "      <td>1.105439</td>\n",
       "      <td>-0.329482</td>\n",
       "      <td>-0.509063</td>\n",
       "      <td>1.582699</td>\n",
       "      <td>2.563358</td>\n",
       "      <td>1.738872</td>\n",
       "      <td>0.941760</td>\n",
       "      <td>0.797298</td>\n",
       "      <td>2.783096</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244190</td>\n",
       "      <td>2.443109</td>\n",
       "      <td>-0.286278</td>\n",
       "      <td>-0.297409</td>\n",
       "      <td>2.320295</td>\n",
       "      <td>5.112877</td>\n",
       "      <td>3.995433</td>\n",
       "      <td>1.620015</td>\n",
       "      <td>2.370444</td>\n",
       "      <td>6.846856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.537556</td>\n",
       "      <td>0.919273</td>\n",
       "      <td>0.442011</td>\n",
       "      <td>0.406453</td>\n",
       "      <td>-1.017686</td>\n",
       "      <td>-0.713542</td>\n",
       "      <td>-0.700684</td>\n",
       "      <td>-0.404686</td>\n",
       "      <td>-1.035476</td>\n",
       "      <td>-0.826124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604849</td>\n",
       "      <td>1.335771</td>\n",
       "      <td>0.492622</td>\n",
       "      <td>0.473611</td>\n",
       "      <td>-0.625477</td>\n",
       "      <td>-0.630828</td>\n",
       "      <td>-0.605872</td>\n",
       "      <td>-0.226210</td>\n",
       "      <td>0.076431</td>\n",
       "      <td>0.031819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.469393</td>\n",
       "      <td>-0.325708</td>\n",
       "      <td>0.479082</td>\n",
       "      <td>0.358672</td>\n",
       "      <td>0.052642</td>\n",
       "      <td>0.471115</td>\n",
       "      <td>0.134849</td>\n",
       "      <td>0.442131</td>\n",
       "      <td>0.110921</td>\n",
       "      <td>-0.280347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859560</td>\n",
       "      <td>0.261002</td>\n",
       "      <td>0.870902</td>\n",
       "      <td>0.735540</td>\n",
       "      <td>0.316995</td>\n",
       "      <td>1.950627</td>\n",
       "      <td>0.596387</td>\n",
       "      <td>1.010951</td>\n",
       "      <td>1.441838</td>\n",
       "      <td>1.155652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.432201</td>\n",
       "      <td>1.282296</td>\n",
       "      <td>1.665360</td>\n",
       "      <td>1.331355</td>\n",
       "      <td>0.073992</td>\n",
       "      <td>2.680858</td>\n",
       "      <td>1.477729</td>\n",
       "      <td>1.621948</td>\n",
       "      <td>2.137194</td>\n",
       "      <td>2.155097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971385</td>\n",
       "      <td>0.694167</td>\n",
       "      <td>1.323647</td>\n",
       "      <td>0.793551</td>\n",
       "      <td>-1.256713</td>\n",
       "      <td>0.865372</td>\n",
       "      <td>0.439988</td>\n",
       "      <td>0.945477</td>\n",
       "      <td>0.445285</td>\n",
       "      <td>1.017112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.489274</td>\n",
       "      <td>1.084495</td>\n",
       "      <td>0.483201</td>\n",
       "      <td>0.363507</td>\n",
       "      <td>-0.878913</td>\n",
       "      <td>-0.078478</td>\n",
       "      <td>0.132840</td>\n",
       "      <td>0.121770</td>\n",
       "      <td>0.129175</td>\n",
       "      <td>-1.335044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118205</td>\n",
       "      <td>0.322883</td>\n",
       "      <td>0.141149</td>\n",
       "      <td>-0.007178</td>\n",
       "      <td>-0.844656</td>\n",
       "      <td>-0.393548</td>\n",
       "      <td>-0.191846</td>\n",
       "      <td>-0.041207</td>\n",
       "      <td>-0.148441</td>\n",
       "      <td>-1.167934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.112836</td>\n",
       "      <td>0.772668</td>\n",
       "      <td>0.067180</td>\n",
       "      <td>-0.217827</td>\n",
       "      <td>1.191289</td>\n",
       "      <td>2.368158</td>\n",
       "      <td>1.556825</td>\n",
       "      <td>0.808147</td>\n",
       "      <td>0.939685</td>\n",
       "      <td>1.987820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256615</td>\n",
       "      <td>1.031253</td>\n",
       "      <td>0.045834</td>\n",
       "      <td>-0.321493</td>\n",
       "      <td>1.434810</td>\n",
       "      <td>3.296698</td>\n",
       "      <td>2.025090</td>\n",
       "      <td>1.616970</td>\n",
       "      <td>1.124753</td>\n",
       "      <td>3.278077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.117215</td>\n",
       "      <td>1.919912</td>\n",
       "      <td>0.196105</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>1.248222</td>\n",
       "      <td>1.045345</td>\n",
       "      <td>0.942887</td>\n",
       "      <td>0.637649</td>\n",
       "      <td>1.794006</td>\n",
       "      <td>1.130169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246596</td>\n",
       "      <td>1.865014</td>\n",
       "      <td>0.501557</td>\n",
       "      <td>0.110075</td>\n",
       "      <td>1.553167</td>\n",
       "      <td>2.566410</td>\n",
       "      <td>2.064909</td>\n",
       "      <td>0.861731</td>\n",
       "      <td>2.131012</td>\n",
       "      <td>2.779335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.156977</td>\n",
       "      <td>0.195555</td>\n",
       "      <td>0.114137</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>0.164372</td>\n",
       "      <td>-0.612909</td>\n",
       "      <td>-0.186433</td>\n",
       "      <td>0.094686</td>\n",
       "      <td>-0.823721</td>\n",
       "      <td>-0.507163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579999</td>\n",
       "      <td>0.847240</td>\n",
       "      <td>0.480707</td>\n",
       "      <td>0.452516</td>\n",
       "      <td>0.615079</td>\n",
       "      <td>-0.427264</td>\n",
       "      <td>0.092168</td>\n",
       "      <td>0.704897</td>\n",
       "      <td>0.207471</td>\n",
       "      <td>-0.098963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.568798</td>\n",
       "      <td>0.323544</td>\n",
       "      <td>0.664438</td>\n",
       "      <td>0.409297</td>\n",
       "      <td>1.468835</td>\n",
       "      <td>1.854573</td>\n",
       "      <td>1.047093</td>\n",
       "      <td>1.389802</td>\n",
       "      <td>1.286524</td>\n",
       "      <td>1.525681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971385</td>\n",
       "      <td>0.944946</td>\n",
       "      <td>0.879838</td>\n",
       "      <td>0.763667</td>\n",
       "      <td>2.039746</td>\n",
       "      <td>1.075298</td>\n",
       "      <td>0.989305</td>\n",
       "      <td>1.411411</td>\n",
       "      <td>1.302709</td>\n",
       "      <td>1.676560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.613970</td>\n",
       "      <td>0.665623</td>\n",
       "      <td>1.566503</td>\n",
       "      <td>1.720997</td>\n",
       "      <td>0.138753</td>\n",
       "      <td>-0.031099</td>\n",
       "      <td>0.742007</td>\n",
       "      <td>1.188093</td>\n",
       "      <td>-0.838325</td>\n",
       "      <td>-1.254241</td>\n",
       "      <td>...</td>\n",
       "      <td>2.288430</td>\n",
       "      <td>0.847240</td>\n",
       "      <td>2.369129</td>\n",
       "      <td>2.667486</td>\n",
       "      <td>0.825491</td>\n",
       "      <td>0.386359</td>\n",
       "      <td>1.271399</td>\n",
       "      <td>1.891049</td>\n",
       "      <td>-0.214770</td>\n",
       "      <td>-0.432012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.166799</td>\n",
       "      <td>-1.147162</td>\n",
       "      <td>-0.185728</td>\n",
       "      <td>-0.251957</td>\n",
       "      <td>0.101747</td>\n",
       "      <td>-0.436850</td>\n",
       "      <td>-0.278210</td>\n",
       "      <td>-0.028609</td>\n",
       "      <td>0.267911</td>\n",
       "      <td>-0.728310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240048</td>\n",
       "      <td>-1.045005</td>\n",
       "      <td>-0.225217</td>\n",
       "      <td>-0.297761</td>\n",
       "      <td>0.509873</td>\n",
       "      <td>-0.489605</td>\n",
       "      <td>-0.159223</td>\n",
       "      <td>0.216123</td>\n",
       "      <td>0.123347</td>\n",
       "      <td>-0.629292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.297446</td>\n",
       "      <td>-0.833008</td>\n",
       "      <td>-0.261106</td>\n",
       "      <td>-0.383638</td>\n",
       "      <td>0.792763</td>\n",
       "      <td>0.429422</td>\n",
       "      <td>-0.541362</td>\n",
       "      <td>-0.459627</td>\n",
       "      <td>0.567289</td>\n",
       "      <td>0.753087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366368</td>\n",
       "      <td>-0.844707</td>\n",
       "      <td>-0.332744</td>\n",
       "      <td>-0.439624</td>\n",
       "      <td>-0.051226</td>\n",
       "      <td>0.148443</td>\n",
       "      <td>-0.399099</td>\n",
       "      <td>-0.636110</td>\n",
       "      <td>0.458227</td>\n",
       "      <td>-0.117250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.313080</td>\n",
       "      <td>-1.593959</td>\n",
       "      <td>-1.302806</td>\n",
       "      <td>-1.083572</td>\n",
       "      <td>0.429819</td>\n",
       "      <td>-0.747086</td>\n",
       "      <td>-0.743748</td>\n",
       "      <td>-0.726337</td>\n",
       "      <td>0.012345</td>\n",
       "      <td>0.886341</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.250611</td>\n",
       "      <td>-1.631243</td>\n",
       "      <td>-1.254913</td>\n",
       "      <td>-0.994422</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>-0.887193</td>\n",
       "      <td>-0.880434</td>\n",
       "      <td>-0.796903</td>\n",
       "      <td>-0.729224</td>\n",
       "      <td>-0.344455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.344426</td>\n",
       "      <td>-1.170433</td>\n",
       "      <td>0.433773</td>\n",
       "      <td>0.140814</td>\n",
       "      <td>0.778530</td>\n",
       "      <td>2.068725</td>\n",
       "      <td>1.492795</td>\n",
       "      <td>1.254641</td>\n",
       "      <td>2.589911</td>\n",
       "      <td>1.066377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372916</td>\n",
       "      <td>-1.074317</td>\n",
       "      <td>0.531343</td>\n",
       "      <td>0.176348</td>\n",
       "      <td>0.290694</td>\n",
       "      <td>2.170095</td>\n",
       "      <td>1.719008</td>\n",
       "      <td>1.898662</td>\n",
       "      <td>2.857396</td>\n",
       "      <td>0.859731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.997389</td>\n",
       "      <td>0.872732</td>\n",
       "      <td>1.863073</td>\n",
       "      <td>2.130548</td>\n",
       "      <td>-0.148044</td>\n",
       "      <td>-0.040575</td>\n",
       "      <td>0.262407</td>\n",
       "      <td>0.964717</td>\n",
       "      <td>-0.155598</td>\n",
       "      <td>-1.420100</td>\n",
       "      <td>...</td>\n",
       "      <td>2.671532</td>\n",
       "      <td>1.614234</td>\n",
       "      <td>2.404872</td>\n",
       "      <td>3.048953</td>\n",
       "      <td>0.338913</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>0.207788</td>\n",
       "      <td>1.313961</td>\n",
       "      <td>-0.127409</td>\n",
       "      <td>-0.481332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.716485</td>\n",
       "      <td>0.486439</td>\n",
       "      <td>0.742699</td>\n",
       "      <td>0.710203</td>\n",
       "      <td>1.120124</td>\n",
       "      <td>0.783815</td>\n",
       "      <td>0.799760</td>\n",
       "      <td>1.103489</td>\n",
       "      <td>0.669515</td>\n",
       "      <td>0.071219</td>\n",
       "      <td>...</td>\n",
       "      <td>2.110339</td>\n",
       "      <td>0.957974</td>\n",
       "      <td>2.077228</td>\n",
       "      <td>2.345788</td>\n",
       "      <td>2.109883</td>\n",
       "      <td>0.658627</td>\n",
       "      <td>0.946607</td>\n",
       "      <td>1.444909</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.648043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.855652</td>\n",
       "      <td>-0.672441</td>\n",
       "      <td>0.989840</td>\n",
       "      <td>0.733241</td>\n",
       "      <td>1.582699</td>\n",
       "      <td>2.335941</td>\n",
       "      <td>1.683630</td>\n",
       "      <td>2.351917</td>\n",
       "      <td>4.484751</td>\n",
       "      <td>1.606484</td>\n",
       "      <td>...</td>\n",
       "      <td>1.238521</td>\n",
       "      <td>-0.696519</td>\n",
       "      <td>1.344497</td>\n",
       "      <td>1.020322</td>\n",
       "      <td>0.970150</td>\n",
       "      <td>0.894635</td>\n",
       "      <td>0.542655</td>\n",
       "      <td>2.137720</td>\n",
       "      <td>1.885110</td>\n",
       "      <td>1.216609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.128576</td>\n",
       "      <td>0.521345</td>\n",
       "      <td>0.224115</td>\n",
       "      <td>-0.028694</td>\n",
       "      <td>0.643316</td>\n",
       "      <td>1.562720</td>\n",
       "      <td>0.674211</td>\n",
       "      <td>1.003666</td>\n",
       "      <td>1.607807</td>\n",
       "      <td>0.913276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279729</td>\n",
       "      <td>1.226666</td>\n",
       "      <td>0.450921</td>\n",
       "      <td>0.028684</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>2.608395</td>\n",
       "      <td>1.351518</td>\n",
       "      <td>2.367641</td>\n",
       "      <td>2.205430</td>\n",
       "      <td>2.413591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.273153</td>\n",
       "      <td>0.223480</td>\n",
       "      <td>1.241101</td>\n",
       "      <td>1.248876</td>\n",
       "      <td>-0.139504</td>\n",
       "      <td>0.042812</td>\n",
       "      <td>0.755818</td>\n",
       "      <td>0.732313</td>\n",
       "      <td>-0.418466</td>\n",
       "      <td>-0.823289</td>\n",
       "      <td>...</td>\n",
       "      <td>1.043864</td>\n",
       "      <td>0.257745</td>\n",
       "      <td>0.972174</td>\n",
       "      <td>0.918363</td>\n",
       "      <td>0.062747</td>\n",
       "      <td>-0.270773</td>\n",
       "      <td>0.347396</td>\n",
       "      <td>0.523700</td>\n",
       "      <td>-0.905562</td>\n",
       "      <td>-0.539518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.333066</td>\n",
       "      <td>1.391668</td>\n",
       "      <td>0.429654</td>\n",
       "      <td>0.220449</td>\n",
       "      <td>0.842579</td>\n",
       "      <td>1.238650</td>\n",
       "      <td>0.998129</td>\n",
       "      <td>0.995412</td>\n",
       "      <td>0.417600</td>\n",
       "      <td>0.368916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828498</td>\n",
       "      <td>1.796619</td>\n",
       "      <td>1.252161</td>\n",
       "      <td>0.682803</td>\n",
       "      <td>1.390974</td>\n",
       "      <td>2.269333</td>\n",
       "      <td>1.733401</td>\n",
       "      <td>1.336800</td>\n",
       "      <td>1.822016</td>\n",
       "      <td>0.820940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.977778</td>\n",
       "      <td>-0.986595</td>\n",
       "      <td>0.948650</td>\n",
       "      <td>0.853831</td>\n",
       "      <td>0.150139</td>\n",
       "      <td>0.215270</td>\n",
       "      <td>0.124931</td>\n",
       "      <td>0.789576</td>\n",
       "      <td>-0.265127</td>\n",
       "      <td>-0.185367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774656</td>\n",
       "      <td>-1.002666</td>\n",
       "      <td>0.823244</td>\n",
       "      <td>0.608971</td>\n",
       "      <td>-0.301091</td>\n",
       "      <td>0.171344</td>\n",
       "      <td>-0.111727</td>\n",
       "      <td>0.471930</td>\n",
       "      <td>-0.234183</td>\n",
       "      <td>-0.263547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>-1.827998</td>\n",
       "      <td>1.431228</td>\n",
       "      <td>-1.797089</td>\n",
       "      <td>-1.377937</td>\n",
       "      <td>-0.688901</td>\n",
       "      <td>0.294866</td>\n",
       "      <td>0.046713</td>\n",
       "      <td>-0.909990</td>\n",
       "      <td>0.822855</td>\n",
       "      <td>2.085634</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.572003</td>\n",
       "      <td>1.011712</td>\n",
       "      <td>-1.571835</td>\n",
       "      <td>-1.154919</td>\n",
       "      <td>1.193713</td>\n",
       "      <td>0.331651</td>\n",
       "      <td>0.321969</td>\n",
       "      <td>-0.983733</td>\n",
       "      <td>-0.179178</td>\n",
       "      <td>1.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>-0.734828</td>\n",
       "      <td>-1.128546</td>\n",
       "      <td>-0.713374</td>\n",
       "      <td>-0.716683</td>\n",
       "      <td>0.247636</td>\n",
       "      <td>0.145150</td>\n",
       "      <td>-0.269044</td>\n",
       "      <td>-0.592724</td>\n",
       "      <td>0.023298</td>\n",
       "      <td>0.711976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.830233</td>\n",
       "      <td>-0.976611</td>\n",
       "      <td>-0.848337</td>\n",
       "      <td>-0.743216</td>\n",
       "      <td>0.093432</td>\n",
       "      <td>-0.270137</td>\n",
       "      <td>-0.443716</td>\n",
       "      <td>-0.691687</td>\n",
       "      <td>-0.924975</td>\n",
       "      <td>-0.144403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>0.097334</td>\n",
       "      <td>1.326510</td>\n",
       "      <td>0.158210</td>\n",
       "      <td>0.004297</td>\n",
       "      <td>-0.568632</td>\n",
       "      <td>0.353616</td>\n",
       "      <td>0.151924</td>\n",
       "      <td>-0.258434</td>\n",
       "      <td>0.220449</td>\n",
       "      <td>0.086813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010186</td>\n",
       "      <td>0.985657</td>\n",
       "      <td>0.185828</td>\n",
       "      <td>-0.126013</td>\n",
       "      <td>0.071514</td>\n",
       "      <td>1.055578</td>\n",
       "      <td>0.632369</td>\n",
       "      <td>0.089742</td>\n",
       "      <td>0.463080</td>\n",
       "      <td>1.017112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0.174018</td>\n",
       "      <td>1.426574</td>\n",
       "      <td>0.112489</td>\n",
       "      <td>0.038995</td>\n",
       "      <td>-0.968582</td>\n",
       "      <td>-0.610256</td>\n",
       "      <td>-0.599491</td>\n",
       "      <td>-0.481036</td>\n",
       "      <td>0.103619</td>\n",
       "      <td>-0.850224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049868</td>\n",
       "      <td>1.076850</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>-0.095249</td>\n",
       "      <td>-1.155891</td>\n",
       "      <td>-0.742153</td>\n",
       "      <td>-0.532950</td>\n",
       "      <td>-0.077750</td>\n",
       "      <td>-0.289188</td>\n",
       "      <td>-0.797202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>-0.260524</td>\n",
       "      <td>2.040920</td>\n",
       "      <td>-0.291999</td>\n",
       "      <td>-0.331307</td>\n",
       "      <td>-0.686767</td>\n",
       "      <td>-0.674123</td>\n",
       "      <td>-0.739856</td>\n",
       "      <td>-0.417067</td>\n",
       "      <td>-0.670381</td>\n",
       "      <td>-0.707046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393289</td>\n",
       "      <td>1.871527</td>\n",
       "      <td>-0.440271</td>\n",
       "      <td>-0.441206</td>\n",
       "      <td>-1.103288</td>\n",
       "      <td>-0.738972</td>\n",
       "      <td>-0.796334</td>\n",
       "      <td>-0.533330</td>\n",
       "      <td>-0.692015</td>\n",
       "      <td>-1.081485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>-0.073075</td>\n",
       "      <td>0.328198</td>\n",
       "      <td>-0.090579</td>\n",
       "      <td>-0.199341</td>\n",
       "      <td>-0.041296</td>\n",
       "      <td>-0.048155</td>\n",
       "      <td>-0.651846</td>\n",
       "      <td>-0.650760</td>\n",
       "      <td>-0.699589</td>\n",
       "      <td>0.578721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252473</td>\n",
       "      <td>-0.150993</td>\n",
       "      <td>-0.241004</td>\n",
       "      <td>-0.337490</td>\n",
       "      <td>-0.261639</td>\n",
       "      <td>-0.321664</td>\n",
       "      <td>-0.645212</td>\n",
       "      <td>-0.702802</td>\n",
       "      <td>-1.054398</td>\n",
       "      <td>0.053985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>-0.144078</td>\n",
       "      <td>0.916946</td>\n",
       "      <td>-0.196849</td>\n",
       "      <td>-0.232332</td>\n",
       "      <td>-0.277565</td>\n",
       "      <td>-0.698760</td>\n",
       "      <td>-0.741488</td>\n",
       "      <td>-0.631673</td>\n",
       "      <td>-0.538947</td>\n",
       "      <td>-0.678694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190348</td>\n",
       "      <td>0.555750</td>\n",
       "      <td>-0.288363</td>\n",
       "      <td>-0.265064</td>\n",
       "      <td>-0.472051</td>\n",
       "      <td>-0.652457</td>\n",
       "      <td>-0.802570</td>\n",
       "      <td>-0.652707</td>\n",
       "      <td>-0.418610</td>\n",
       "      <td>-0.798864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>-1.081325</td>\n",
       "      <td>-0.684076</td>\n",
       "      <td>-1.098091</td>\n",
       "      <td>-0.938523</td>\n",
       "      <td>-0.143774</td>\n",
       "      <td>-1.030979</td>\n",
       "      <td>-0.987817</td>\n",
       "      <td>-1.120082</td>\n",
       "      <td>0.267911</td>\n",
       "      <td>-0.111652</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.039387</td>\n",
       "      <td>-0.636267</td>\n",
       "      <td>-1.076496</td>\n",
       "      <td>-0.871368</td>\n",
       "      <td>-0.169583</td>\n",
       "      <td>-1.055006</td>\n",
       "      <td>-1.095507</td>\n",
       "      <td>-1.382518</td>\n",
       "      <td>-0.355517</td>\n",
       "      <td>-0.551710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>-1.098366</td>\n",
       "      <td>-0.630553</td>\n",
       "      <td>-1.075848</td>\n",
       "      <td>-0.950184</td>\n",
       "      <td>-0.540166</td>\n",
       "      <td>-0.448790</td>\n",
       "      <td>-0.567727</td>\n",
       "      <td>-0.632962</td>\n",
       "      <td>-0.520693</td>\n",
       "      <td>0.615579</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.126361</td>\n",
       "      <td>-0.592299</td>\n",
       "      <td>-1.077688</td>\n",
       "      <td>-0.919710</td>\n",
       "      <td>0.601928</td>\n",
       "      <td>-0.188711</td>\n",
       "      <td>-0.450432</td>\n",
       "      <td>-0.476230</td>\n",
       "      <td>-0.339339</td>\n",
       "      <td>0.600939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>-1.262242</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>-1.273561</td>\n",
       "      <td>-1.050012</td>\n",
       "      <td>-0.814864</td>\n",
       "      <td>-1.024157</td>\n",
       "      <td>-0.821463</td>\n",
       "      <td>-1.013810</td>\n",
       "      <td>-0.845627</td>\n",
       "      <td>-0.063453</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.105653</td>\n",
       "      <td>-0.014204</td>\n",
       "      <td>-1.136664</td>\n",
       "      <td>-0.907756</td>\n",
       "      <td>-0.546572</td>\n",
       "      <td>-1.010222</td>\n",
       "      <td>-0.857262</td>\n",
       "      <td>-1.159448</td>\n",
       "      <td>-0.564210</td>\n",
       "      <td>-0.262993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>-0.939318</td>\n",
       "      <td>1.144999</td>\n",
       "      <td>-0.950630</td>\n",
       "      <td>-0.834144</td>\n",
       "      <td>-1.027649</td>\n",
       "      <td>-0.726239</td>\n",
       "      <td>-0.920522</td>\n",
       "      <td>-1.051341</td>\n",
       "      <td>0.600147</td>\n",
       "      <td>0.068384</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.670780</td>\n",
       "      <td>0.940061</td>\n",
       "      <td>-0.695833</td>\n",
       "      <td>-0.659188</td>\n",
       "      <td>-0.524654</td>\n",
       "      <td>-0.578665</td>\n",
       "      <td>-1.008672</td>\n",
       "      <td>-1.248067</td>\n",
       "      <td>0.256005</td>\n",
       "      <td>-0.425916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>-0.927957</td>\n",
       "      <td>0.509709</td>\n",
       "      <td>-0.966282</td>\n",
       "      <td>-0.837273</td>\n",
       "      <td>-1.569218</td>\n",
       "      <td>-1.176337</td>\n",
       "      <td>-1.114873</td>\n",
       "      <td>-1.261820</td>\n",
       "      <td>-0.549900</td>\n",
       "      <td>-0.470306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954483</td>\n",
       "      <td>-0.147736</td>\n",
       "      <td>-0.988330</td>\n",
       "      <td>-0.823201</td>\n",
       "      <td>-1.414523</td>\n",
       "      <td>-1.150045</td>\n",
       "      <td>-1.305831</td>\n",
       "      <td>-1.745063</td>\n",
       "      <td>-0.716282</td>\n",
       "      <td>-0.998915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>-0.851273</td>\n",
       "      <td>0.733108</td>\n",
       "      <td>-0.843535</td>\n",
       "      <td>-0.786363</td>\n",
       "      <td>-0.049836</td>\n",
       "      <td>-0.424532</td>\n",
       "      <td>-0.509221</td>\n",
       "      <td>-0.679649</td>\n",
       "      <td>0.797298</td>\n",
       "      <td>0.385927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.879933</td>\n",
       "      <td>0.420589</td>\n",
       "      <td>-0.877527</td>\n",
       "      <td>-0.780484</td>\n",
       "      <td>-1.037534</td>\n",
       "      <td>-0.483880</td>\n",
       "      <td>-0.555498</td>\n",
       "      <td>-0.768581</td>\n",
       "      <td>0.433960</td>\n",
       "      <td>-0.200928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>-0.385490</td>\n",
       "      <td>2.359728</td>\n",
       "      <td>-0.437400</td>\n",
       "      <td>-0.418052</td>\n",
       "      <td>-0.967870</td>\n",
       "      <td>-1.175010</td>\n",
       "      <td>-0.864150</td>\n",
       "      <td>-0.875168</td>\n",
       "      <td>-0.995315</td>\n",
       "      <td>-0.911181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496830</td>\n",
       "      <td>1.681000</td>\n",
       "      <td>-0.570733</td>\n",
       "      <td>-0.502558</td>\n",
       "      <td>-0.393146</td>\n",
       "      <td>-0.940628</td>\n",
       "      <td>-0.890701</td>\n",
       "      <td>-0.755639</td>\n",
       "      <td>-0.798788</td>\n",
       "      <td>-1.058764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>-1.361647</td>\n",
       "      <td>0.616755</td>\n",
       "      <td>-1.357589</td>\n",
       "      <td>-1.111729</td>\n",
       "      <td>-0.281835</td>\n",
       "      <td>-0.915186</td>\n",
       "      <td>-0.613176</td>\n",
       "      <td>-0.931141</td>\n",
       "      <td>-0.436721</td>\n",
       "      <td>0.419950</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.330337</td>\n",
       "      <td>-0.102139</td>\n",
       "      <td>-1.322527</td>\n",
       "      <td>-1.027998</td>\n",
       "      <td>-0.967396</td>\n",
       "      <td>-1.089612</td>\n",
       "      <td>-0.922365</td>\n",
       "      <td>-1.354653</td>\n",
       "      <td>-0.753491</td>\n",
       "      <td>-0.555035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>-0.354249</td>\n",
       "      <td>2.241047</td>\n",
       "      <td>-0.390031</td>\n",
       "      <td>-0.399850</td>\n",
       "      <td>-1.076753</td>\n",
       "      <td>-0.873682</td>\n",
       "      <td>-0.337092</td>\n",
       "      <td>-0.657467</td>\n",
       "      <td>-0.896740</td>\n",
       "      <td>-0.810531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.492689</td>\n",
       "      <td>1.638661</td>\n",
       "      <td>-0.548691</td>\n",
       "      <td>-0.500800</td>\n",
       "      <td>-0.423831</td>\n",
       "      <td>-0.586935</td>\n",
       "      <td>-0.135715</td>\n",
       "      <td>-0.756400</td>\n",
       "      <td>-0.855411</td>\n",
       "      <td>-0.638713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>-1.089845</td>\n",
       "      <td>1.936202</td>\n",
       "      <td>-1.083262</td>\n",
       "      <td>-0.948477</td>\n",
       "      <td>-0.431283</td>\n",
       "      <td>-0.526112</td>\n",
       "      <td>-0.361700</td>\n",
       "      <td>-0.555580</td>\n",
       "      <td>-0.798164</td>\n",
       "      <td>-0.216555</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.124290</td>\n",
       "      <td>1.503500</td>\n",
       "      <td>-1.122664</td>\n",
       "      <td>-0.919359</td>\n",
       "      <td>0.264392</td>\n",
       "      <td>-0.529682</td>\n",
       "      <td>-0.346326</td>\n",
       "      <td>-0.355331</td>\n",
       "      <td>-1.091607</td>\n",
       "      <td>-0.061834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>-1.126767</td>\n",
       "      <td>0.069894</td>\n",
       "      <td>-1.121981</td>\n",
       "      <td>-0.976065</td>\n",
       "      <td>0.280372</td>\n",
       "      <td>-0.555297</td>\n",
       "      <td>-1.051784</td>\n",
       "      <td>-0.973959</td>\n",
       "      <td>-0.075277</td>\n",
       "      <td>0.072637</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.163636</td>\n",
       "      <td>-0.455510</td>\n",
       "      <td>-1.173002</td>\n",
       "      <td>-0.937465</td>\n",
       "      <td>-0.257255</td>\n",
       "      <td>-0.854113</td>\n",
       "      <td>-1.257616</td>\n",
       "      <td>-1.405205</td>\n",
       "      <td>-1.033367</td>\n",
       "      <td>-0.915792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>-1.336086</td>\n",
       "      <td>1.999032</td>\n",
       "      <td>-1.347292</td>\n",
       "      <td>-1.090967</td>\n",
       "      <td>-1.076753</td>\n",
       "      <td>-1.035338</td>\n",
       "      <td>-1.114873</td>\n",
       "      <td>-1.261820</td>\n",
       "      <td>-0.254174</td>\n",
       "      <td>-0.312952</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.196769</td>\n",
       "      <td>1.394395</td>\n",
       "      <td>-1.214107</td>\n",
       "      <td>-0.966822</td>\n",
       "      <td>-1.098904</td>\n",
       "      <td>-1.162132</td>\n",
       "      <td>-1.305831</td>\n",
       "      <td>-1.745063</td>\n",
       "      <td>-0.688779</td>\n",
       "      <td>-0.789998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0.131416</td>\n",
       "      <td>0.788958</td>\n",
       "      <td>0.182100</td>\n",
       "      <td>0.006288</td>\n",
       "      <td>-0.827674</td>\n",
       "      <td>0.543131</td>\n",
       "      <td>0.177034</td>\n",
       "      <td>-0.298156</td>\n",
       "      <td>-1.305645</td>\n",
       "      <td>-0.188203</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163427</td>\n",
       "      <td>0.259374</td>\n",
       "      <td>-0.040545</td>\n",
       "      <td>-0.258559</td>\n",
       "      <td>-1.304933</td>\n",
       "      <td>0.399718</td>\n",
       "      <td>0.451022</td>\n",
       "      <td>-0.062524</td>\n",
       "      <td>-1.039838</td>\n",
       "      <td>-0.216444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>-0.743348</td>\n",
       "      <td>1.079841</td>\n",
       "      <td>-0.718729</td>\n",
       "      <td>-0.714976</td>\n",
       "      <td>-0.266890</td>\n",
       "      <td>-0.042470</td>\n",
       "      <td>0.281240</td>\n",
       "      <td>-0.202977</td>\n",
       "      <td>-1.546608</td>\n",
       "      <td>0.411444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.784675</td>\n",
       "      <td>1.869899</td>\n",
       "      <td>-0.744086</td>\n",
       "      <td>-0.714386</td>\n",
       "      <td>-0.112597</td>\n",
       "      <td>-0.016317</td>\n",
       "      <td>0.435670</td>\n",
       "      <td>-0.275239</td>\n",
       "      <td>-1.276034</td>\n",
       "      <td>0.186983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>-0.021952</td>\n",
       "      <td>1.829157</td>\n",
       "      <td>-0.024262</td>\n",
       "      <td>-0.154973</td>\n",
       "      <td>0.208495</td>\n",
       "      <td>0.156521</td>\n",
       "      <td>-0.554670</td>\n",
       "      <td>-0.151647</td>\n",
       "      <td>-1.002617</td>\n",
       "      <td>-0.154180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200702</td>\n",
       "      <td>1.220152</td>\n",
       "      <td>-0.210324</td>\n",
       "      <td>-0.305671</td>\n",
       "      <td>-0.362461</td>\n",
       "      <td>-0.177261</td>\n",
       "      <td>-0.669679</td>\n",
       "      <td>-0.149315</td>\n",
       "      <td>-1.052780</td>\n",
       "      <td>-0.040776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>-0.831392</td>\n",
       "      <td>2.345765</td>\n",
       "      <td>-0.877311</td>\n",
       "      <td>-0.764748</td>\n",
       "      <td>-1.556408</td>\n",
       "      <td>-1.303122</td>\n",
       "      <td>-1.114873</td>\n",
       "      <td>-1.261820</td>\n",
       "      <td>-2.744117</td>\n",
       "      <td>-1.102557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900641</td>\n",
       "      <td>2.055541</td>\n",
       "      <td>-0.955268</td>\n",
       "      <td>-0.775210</td>\n",
       "      <td>-1.740223</td>\n",
       "      <td>-1.267986</td>\n",
       "      <td>-1.305831</td>\n",
       "      <td>-1.745063</td>\n",
       "      <td>-2.159342</td>\n",
       "      <td>-1.379622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0.310345</td>\n",
       "      <td>2.636649</td>\n",
       "      <td>0.470844</td>\n",
       "      <td>0.176365</td>\n",
       "      <td>0.600616</td>\n",
       "      <td>1.977758</td>\n",
       "      <td>2.086645</td>\n",
       "      <td>1.170295</td>\n",
       "      <td>1.155090</td>\n",
       "      <td>1.236490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259021</td>\n",
       "      <td>2.786709</td>\n",
       "      <td>0.638572</td>\n",
       "      <td>0.060502</td>\n",
       "      <td>0.409050</td>\n",
       "      <td>3.418837</td>\n",
       "      <td>4.307272</td>\n",
       "      <td>1.842324</td>\n",
       "      <td>1.922319</td>\n",
       "      <td>3.156163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1.929226</td>\n",
       "      <td>1.349781</td>\n",
       "      <td>2.101976</td>\n",
       "      <td>1.968434</td>\n",
       "      <td>0.963560</td>\n",
       "      <td>2.260135</td>\n",
       "      <td>2.870075</td>\n",
       "      <td>2.540213</td>\n",
       "      <td>1.231760</td>\n",
       "      <td>0.849484</td>\n",
       "      <td>...</td>\n",
       "      <td>1.660970</td>\n",
       "      <td>0.607860</td>\n",
       "      <td>2.139779</td>\n",
       "      <td>1.649655</td>\n",
       "      <td>0.365215</td>\n",
       "      <td>1.045400</td>\n",
       "      <td>1.860055</td>\n",
       "      <td>2.125538</td>\n",
       "      <td>0.045693</td>\n",
       "      <td>0.819278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>2.110995</td>\n",
       "      <td>0.721473</td>\n",
       "      <td>2.060786</td>\n",
       "      <td>2.343856</td>\n",
       "      <td>1.041842</td>\n",
       "      <td>0.219060</td>\n",
       "      <td>1.947285</td>\n",
       "      <td>2.320965</td>\n",
       "      <td>-0.312589</td>\n",
       "      <td>-0.931027</td>\n",
       "      <td>...</td>\n",
       "      <td>1.901185</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>1.752563</td>\n",
       "      <td>2.015301</td>\n",
       "      <td>0.378365</td>\n",
       "      <td>-0.273318</td>\n",
       "      <td>0.664512</td>\n",
       "      <td>1.629151</td>\n",
       "      <td>-1.360158</td>\n",
       "      <td>-0.709091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1.704854</td>\n",
       "      <td>2.085134</td>\n",
       "      <td>1.615931</td>\n",
       "      <td>1.723842</td>\n",
       "      <td>0.102458</td>\n",
       "      <td>-0.017833</td>\n",
       "      <td>0.693043</td>\n",
       "      <td>1.263669</td>\n",
       "      <td>-0.217664</td>\n",
       "      <td>-1.058611</td>\n",
       "      <td>...</td>\n",
       "      <td>1.536720</td>\n",
       "      <td>2.047399</td>\n",
       "      <td>1.421940</td>\n",
       "      <td>1.494959</td>\n",
       "      <td>-0.691230</td>\n",
       "      <td>-0.394820</td>\n",
       "      <td>0.236573</td>\n",
       "      <td>0.733827</td>\n",
       "      <td>-0.531855</td>\n",
       "      <td>-0.973978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.702284</td>\n",
       "      <td>2.045574</td>\n",
       "      <td>0.672676</td>\n",
       "      <td>0.577953</td>\n",
       "      <td>-0.840484</td>\n",
       "      <td>-0.038680</td>\n",
       "      <td>0.046588</td>\n",
       "      <td>0.105777</td>\n",
       "      <td>-0.809117</td>\n",
       "      <td>-0.895587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561361</td>\n",
       "      <td>1.374854</td>\n",
       "      <td>0.579001</td>\n",
       "      <td>0.427906</td>\n",
       "      <td>-0.809587</td>\n",
       "      <td>0.350735</td>\n",
       "      <td>0.326767</td>\n",
       "      <td>0.414069</td>\n",
       "      <td>-1.104549</td>\n",
       "      <td>-0.318409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1.838341</td>\n",
       "      <td>2.336457</td>\n",
       "      <td>1.982524</td>\n",
       "      <td>1.735218</td>\n",
       "      <td>1.525767</td>\n",
       "      <td>3.272144</td>\n",
       "      <td>3.296944</td>\n",
       "      <td>2.658866</td>\n",
       "      <td>2.137194</td>\n",
       "      <td>1.043695</td>\n",
       "      <td>...</td>\n",
       "      <td>1.961239</td>\n",
       "      <td>2.237926</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>1.653171</td>\n",
       "      <td>1.430427</td>\n",
       "      <td>3.904848</td>\n",
       "      <td>3.197605</td>\n",
       "      <td>2.289985</td>\n",
       "      <td>1.919083</td>\n",
       "      <td>2.219635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-1.808401</td>\n",
       "      <td>1.221792</td>\n",
       "      <td>-1.814389</td>\n",
       "      <td>-1.347789</td>\n",
       "      <td>-3.112085</td>\n",
       "      <td>-1.150752</td>\n",
       "      <td>-1.114873</td>\n",
       "      <td>-1.261820</td>\n",
       "      <td>-0.820070</td>\n",
       "      <td>-0.561032</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.410893</td>\n",
       "      <td>0.764190</td>\n",
       "      <td>-1.432735</td>\n",
       "      <td>-1.075813</td>\n",
       "      <td>-1.859019</td>\n",
       "      <td>-1.207552</td>\n",
       "      <td>-1.305831</td>\n",
       "      <td>-1.745063</td>\n",
       "      <td>-0.048138</td>\n",
       "      <td>-0.751207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    1.097064 -2.073335  1.269934  0.984375  1.568466  3.283515  2.652874   \n",
       "1    1.829821 -0.353632  1.685955  1.908708 -0.826962 -0.487072 -0.023846   \n",
       "2    1.579888  0.456187  1.566503  1.558884  0.942210  1.052926  1.363478   \n",
       "3   -0.768909  0.253732 -0.592687 -0.764464  3.283553  3.402909  1.915897   \n",
       "4    1.750297 -1.151816  1.776573  1.826229  0.280372  0.539340  1.371011   \n",
       "5   -0.476375 -0.835335 -0.387148 -0.505650  2.237421  1.244335  0.866302   \n",
       "6    1.170908  0.160649  1.138125  1.095295 -0.123136  0.088295  0.300072   \n",
       "7   -0.118517  0.358450 -0.072867 -0.218965  1.604049  1.140102  0.061026   \n",
       "8   -0.320167  0.588830 -0.184080 -0.384207  2.201839  1.684010  1.219096   \n",
       "9   -0.473535  1.105439 -0.329482 -0.509063  1.582699  2.563358  1.738872   \n",
       "10   0.537556  0.919273  0.442011  0.406453 -1.017686 -0.713542 -0.700684   \n",
       "11   0.469393 -0.325708  0.479082  0.358672  0.052642  0.471115  0.134849   \n",
       "12   1.432201  1.282296  1.665360  1.331355  0.073992  2.680858  1.477729   \n",
       "13   0.489274  1.084495  0.483201  0.363507 -0.878913 -0.078478  0.132840   \n",
       "14  -0.112836  0.772668  0.067180 -0.217827  1.191289  2.368158  1.556825   \n",
       "15   0.117215  1.919912  0.196105  0.011123  1.248222  1.045345  0.942887   \n",
       "16   0.156977  0.195555  0.114137  0.084216  0.164372 -0.612909 -0.186433   \n",
       "17   0.568798  0.323544  0.664438  0.409297  1.468835  1.854573  1.047093   \n",
       "18   1.613970  0.665623  1.566503  1.720997  0.138753 -0.031099  0.742007   \n",
       "19  -0.166799 -1.147162 -0.185728 -0.251957  0.101747 -0.436850 -0.278210   \n",
       "20  -0.297446 -0.833008 -0.261106 -0.383638  0.792763  0.429422 -0.541362   \n",
       "21  -1.313080 -1.593959 -1.302806 -1.083572  0.429819 -0.747086 -0.743748   \n",
       "22   0.344426 -1.170433  0.433773  0.140814  0.778530  2.068725  1.492795   \n",
       "23   1.997389  0.872732  1.863073  2.130548 -0.148044 -0.040575  0.262407   \n",
       "24   0.716485  0.486439  0.742699  0.710203  1.120124  0.783815  0.799760   \n",
       "25   0.855652 -0.672441  0.989840  0.733241  1.582699  2.335941  1.683630   \n",
       "26   0.128576  0.521345  0.224115 -0.028694  0.643316  1.562720  0.674211   \n",
       "27   1.273153  0.223480  1.241101  1.248876 -0.139504  0.042812  0.755818   \n",
       "28   0.333066  1.391668  0.429654  0.220449  0.842579  1.238650  0.998129   \n",
       "29   0.977778 -0.986595  0.948650  0.853831  0.150139  0.215270  0.124931   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "539 -1.827998  1.431228 -1.797089 -1.377937 -0.688901  0.294866  0.046713   \n",
       "540 -0.734828 -1.128546 -0.713374 -0.716683  0.247636  0.145150 -0.269044   \n",
       "541  0.097334  1.326510  0.158210  0.004297 -0.568632  0.353616  0.151924   \n",
       "542  0.174018  1.426574  0.112489  0.038995 -0.968582 -0.610256 -0.599491   \n",
       "543 -0.260524  2.040920 -0.291999 -0.331307 -0.686767 -0.674123 -0.739856   \n",
       "544 -0.073075  0.328198 -0.090579 -0.199341 -0.041296 -0.048155 -0.651846   \n",
       "545 -0.144078  0.916946 -0.196849 -0.232332 -0.277565 -0.698760 -0.741488   \n",
       "546 -1.081325 -0.684076 -1.098091 -0.938523 -0.143774 -1.030979 -0.987817   \n",
       "547 -1.098366 -0.630553 -1.075848 -0.950184 -0.540166 -0.448790 -0.567727   \n",
       "548 -1.262242  0.011717 -1.273561 -1.050012 -0.814864 -1.024157 -0.821463   \n",
       "549 -0.939318  1.144999 -0.950630 -0.834144 -1.027649 -0.726239 -0.920522   \n",
       "550 -0.927957  0.509709 -0.966282 -0.837273 -1.569218 -1.176337 -1.114873   \n",
       "551 -0.851273  0.733108 -0.843535 -0.786363 -0.049836 -0.424532 -0.509221   \n",
       "552 -0.385490  2.359728 -0.437400 -0.418052 -0.967870 -1.175010 -0.864150   \n",
       "553 -1.361647  0.616755 -1.357589 -1.111729 -0.281835 -0.915186 -0.613176   \n",
       "554 -0.354249  2.241047 -0.390031 -0.399850 -1.076753 -0.873682 -0.337092   \n",
       "555 -1.089845  1.936202 -1.083262 -0.948477 -0.431283 -0.526112 -0.361700   \n",
       "556 -1.126767  0.069894 -1.121981 -0.976065  0.280372 -0.555297 -1.051784   \n",
       "557 -1.336086  1.999032 -1.347292 -1.090967 -1.076753 -1.035338 -1.114873   \n",
       "558  0.131416  0.788958  0.182100  0.006288 -0.827674  0.543131  0.177034   \n",
       "559 -0.743348  1.079841 -0.718729 -0.714976 -0.266890 -0.042470  0.281240   \n",
       "560 -0.021952  1.829157 -0.024262 -0.154973  0.208495  0.156521 -0.554670   \n",
       "561 -0.831392  2.345765 -0.877311 -0.764748 -1.556408 -1.303122 -1.114873   \n",
       "562  0.310345  2.636649  0.470844  0.176365  0.600616  1.977758  2.086645   \n",
       "563  1.929226  1.349781  2.101976  1.968434  0.963560  2.260135  2.870075   \n",
       "564  2.110995  0.721473  2.060786  2.343856  1.041842  0.219060  1.947285   \n",
       "565  1.704854  2.085134  1.615931  1.723842  0.102458 -0.017833  0.693043   \n",
       "566  0.702284  2.045574  0.672676  0.577953 -0.840484 -0.038680  0.046588   \n",
       "567  1.838341  2.336457  1.982524  1.735218  1.525767  3.272144  3.296944   \n",
       "568 -1.808401  1.221792 -1.814389 -1.347789 -3.112085 -1.150752 -1.114873   \n",
       "\n",
       "           7         8         9     ...           20        21        22  \\\n",
       "0    2.532475  2.217515  2.255747    ...     1.886690 -1.359293  2.303601   \n",
       "1    0.548144  0.001392 -0.868652    ...     1.805927 -0.369203  1.535126   \n",
       "2    2.037231  0.939685 -0.398008    ...     1.511870 -0.023974  1.347475   \n",
       "3    1.451707  2.867383  4.910919    ...    -0.281464  0.133984 -0.249939   \n",
       "4    1.428493 -0.009560 -0.562450    ...     1.298575 -1.466770  1.338539   \n",
       "5    0.824656  1.005402  1.890005    ...    -0.165498 -0.313836 -0.115009   \n",
       "6    0.646935 -0.064325 -0.762332    ...     1.368983  0.322883  1.368325   \n",
       "7    0.281950  1.403355  1.660353    ...     0.163763  0.401048  0.099449   \n",
       "8    1.150692  1.965600  1.572462    ...    -0.161357  0.822813 -0.031609   \n",
       "9    0.941760  0.797298  2.783096    ...    -0.244190  2.443109 -0.286278   \n",
       "10  -0.404686 -1.035476 -0.826124    ...     0.604849  1.335771  0.492622   \n",
       "11   0.442131  0.110921 -0.280347    ...     0.859560  0.261002  0.870902   \n",
       "12   1.621948  2.137194  2.155097    ...     0.971385  0.694167  1.323647   \n",
       "13   0.121770  0.129175 -1.335044    ...     0.118205  0.322883  0.141149   \n",
       "14   0.808147  0.939685  1.987820    ...    -0.256615  1.031253  0.045834   \n",
       "15   0.637649  1.794006  1.130169    ...     0.246596  1.865014  0.501557   \n",
       "16   0.094686 -0.823721 -0.507163    ...     0.579999  0.847240  0.480707   \n",
       "17   1.389802  1.286524  1.525681    ...     0.971385  0.944946  0.879838   \n",
       "18   1.188093 -0.838325 -1.254241    ...     2.288430  0.847240  2.369129   \n",
       "19  -0.028609  0.267911 -0.728310    ...    -0.240048 -1.045005 -0.225217   \n",
       "20  -0.459627  0.567289  0.753087    ...    -0.366368 -0.844707 -0.332744   \n",
       "21  -0.726337  0.012345  0.886341    ...    -1.250611 -1.631243 -1.254913   \n",
       "22   1.254641  2.589911  1.066377    ...     0.372916 -1.074317  0.531343   \n",
       "23   0.964717 -0.155598 -1.420100    ...     2.671532  1.614234  2.404872   \n",
       "24   1.103489  0.669515  0.071219    ...     2.110339  0.957974  2.077228   \n",
       "25   2.351917  4.484751  1.606484    ...     1.238521 -0.696519  1.344497   \n",
       "26   1.003666  1.607807  0.913276    ...     0.279729  1.226666  0.450921   \n",
       "27   0.732313 -0.418466 -0.823289    ...     1.043864  0.257745  0.972174   \n",
       "28   0.995412  0.417600  0.368916    ...     0.828498  1.796619  1.252161   \n",
       "29   0.789576 -0.265127 -0.185367    ...     0.774656 -1.002666  0.823244   \n",
       "..        ...       ...       ...    ...          ...       ...       ...   \n",
       "539 -0.909990  0.822855  2.085634    ...    -1.572003  1.011712 -1.571835   \n",
       "540 -0.592724  0.023298  0.711976    ...    -0.830233 -0.976611 -0.848337   \n",
       "541 -0.258434  0.220449  0.086813    ...    -0.010186  0.985657  0.185828   \n",
       "542 -0.481036  0.103619 -0.850224    ...     0.049868  1.076850  0.004134   \n",
       "543 -0.417067 -0.670381 -0.707046    ...    -0.393289  1.871527 -0.440271   \n",
       "544 -0.650760 -0.699589  0.578721    ...    -0.252473 -0.150993 -0.241004   \n",
       "545 -0.631673 -0.538947 -0.678694    ...    -0.190348  0.555750 -0.288363   \n",
       "546 -1.120082  0.267911 -0.111652    ...    -1.039387 -0.636267 -1.076496   \n",
       "547 -0.632962 -0.520693  0.615579    ...    -1.126361 -0.592299 -1.077688   \n",
       "548 -1.013810 -0.845627 -0.063453    ...    -1.105653 -0.014204 -1.136664   \n",
       "549 -1.051341  0.600147  0.068384    ...    -0.670780  0.940061 -0.695833   \n",
       "550 -1.261820 -0.549900 -0.470306    ...    -0.954483 -0.147736 -0.988330   \n",
       "551 -0.679649  0.797298  0.385927    ...    -0.879933  0.420589 -0.877527   \n",
       "552 -0.875168 -0.995315 -0.911181    ...    -0.496830  1.681000 -0.570733   \n",
       "553 -0.931141 -0.436721  0.419950    ...    -1.330337 -0.102139 -1.322527   \n",
       "554 -0.657467 -0.896740 -0.810531    ...    -0.492689  1.638661 -0.548691   \n",
       "555 -0.555580 -0.798164 -0.216555    ...    -1.124290  1.503500 -1.122664   \n",
       "556 -0.973959 -0.075277  0.072637    ...    -1.163636 -0.455510 -1.173002   \n",
       "557 -1.261820 -0.254174 -0.312952    ...    -1.196769  1.394395 -1.214107   \n",
       "558 -0.298156 -1.305645 -0.188203    ...    -0.163427  0.259374 -0.040545   \n",
       "559 -0.202977 -1.546608  0.411444    ...    -0.784675  1.869899 -0.744086   \n",
       "560 -0.151647 -1.002617 -0.154180    ...    -0.200702  1.220152 -0.210324   \n",
       "561 -1.261820 -2.744117 -1.102557    ...    -0.900641  2.055541 -0.955268   \n",
       "562  1.170295  1.155090  1.236490    ...     0.259021  2.786709  0.638572   \n",
       "563  2.540213  1.231760  0.849484    ...     1.660970  0.607860  2.139779   \n",
       "564  2.320965 -0.312589 -0.931027    ...     1.901185  0.117700  1.752563   \n",
       "565  1.263669 -0.217664 -1.058611    ...     1.536720  2.047399  1.421940   \n",
       "566  0.105777 -0.809117 -0.895587    ...     0.561361  1.374854  0.579001   \n",
       "567  2.658866  2.137194  1.043695    ...     1.961239  2.237926  2.303601   \n",
       "568 -1.261820 -0.820070 -0.561032    ...    -1.410893  0.764190 -1.432735   \n",
       "\n",
       "           23        24        25        26        27        28        29  \n",
       "0    2.001237  1.307686  2.616665  2.109526  2.296076  2.750622  1.937015  \n",
       "1    1.890489 -0.375612 -0.430444 -0.146749  1.087084 -0.243890  0.281190  \n",
       "2    1.456285  0.527407  1.082932  0.854974  1.955000  1.152255  0.201391  \n",
       "3   -0.550021  3.394275  3.893397  1.989588  2.175786  6.046041  4.935010  \n",
       "4    1.220724  0.220556 -0.313395  0.613179  0.729259 -0.868353 -0.397100  \n",
       "5   -0.244320  2.048513  1.721616  1.263243  0.905888  1.754069  2.241802  \n",
       "6    1.275220  0.518640  0.021215  0.509552  1.196716  0.262476 -0.014730  \n",
       "7    0.028859  1.447961  0.724786 -0.021054  0.624196  0.477640  1.726435  \n",
       "8   -0.248363  1.662757  1.818310  1.280035  1.391616  2.389857  1.288650  \n",
       "9   -0.297409  2.320295  5.112877  3.995433  1.620015  2.370444  6.846856  \n",
       "10   0.473611 -0.625477 -0.630828 -0.605872 -0.226210  0.076431  0.031819  \n",
       "11   0.735540  0.316995  1.950627  0.596387  1.010951  1.441838  1.155652  \n",
       "12   0.793551 -1.256713  0.865372  0.439988  0.945477  0.445285  1.017112  \n",
       "13  -0.007178 -0.844656 -0.393548 -0.191846 -0.041207 -0.148441 -1.167934  \n",
       "14  -0.321493  1.434810  3.296698  2.025090  1.616970  1.124753  3.278077  \n",
       "15   0.110075  1.553167  2.566410  2.064909  0.861731  2.131012  2.779335  \n",
       "16   0.452516  0.615079 -0.427264  0.092168  0.704897  0.207471 -0.098963  \n",
       "17   0.763667  2.039746  1.075298  0.989305  1.411411  1.302709  1.676560  \n",
       "18   2.667486  0.825491  0.386359  1.271399  1.891049 -0.214770 -0.432012  \n",
       "19  -0.297761  0.509873 -0.489605 -0.159223  0.216123  0.123347 -0.629292  \n",
       "20  -0.439624 -0.051226  0.148443 -0.399099 -0.636110  0.458227 -0.117250  \n",
       "21  -0.994422  0.001377 -0.887193 -0.880434 -0.796903 -0.729224 -0.344455  \n",
       "22   0.176348  0.290694  2.170095  1.719008  1.898662  2.857396  0.859731  \n",
       "23   3.048953  0.338913  0.036482  0.207788  1.313961 -0.127409 -0.481332  \n",
       "24   2.345788  2.109883  0.658627  0.946607  1.444909  1.152255  0.648043  \n",
       "25   1.020322  0.970150  0.894635  0.542655  2.137720  1.885110  1.216609  \n",
       "26   0.028684  0.882478  2.608395  1.351518  2.367641  2.205430  2.413591  \n",
       "27   0.918363  0.062747 -0.270773  0.347396  0.523700 -0.905562 -0.539518  \n",
       "28   0.682803  1.390974  2.269333  1.733401  1.336800  1.822016  0.820940  \n",
       "29   0.608971 -0.301091  0.171344 -0.111727  0.471930 -0.234183 -0.263547  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "539 -1.154919  1.193713  0.331651  0.321969 -0.983733 -0.179178  1.255400  \n",
       "540 -0.743216  0.093432 -0.270137 -0.443716 -0.691687 -0.924975 -0.144403  \n",
       "541 -0.126013  0.071514  1.055578  0.632369  0.089742  0.463080  1.017112  \n",
       "542 -0.095249 -1.155891 -0.742153 -0.532950 -0.077750 -0.289188 -0.797202  \n",
       "543 -0.441206 -1.103288 -0.738972 -0.796334 -0.533330 -0.692015 -1.081485  \n",
       "544 -0.337490 -0.261639 -0.321664 -0.645212 -0.702802 -1.054398  0.053985  \n",
       "545 -0.265064 -0.472051 -0.652457 -0.802570 -0.652707 -0.418610 -0.798864  \n",
       "546 -0.871368 -0.169583 -1.055006 -1.095507 -1.382518 -0.355517 -0.551710  \n",
       "547 -0.919710  0.601928 -0.188711 -0.450432 -0.476230 -0.339339  0.600939  \n",
       "548 -0.907756 -0.546572 -1.010222 -0.857262 -1.159448 -0.564210 -0.262993  \n",
       "549 -0.659188 -0.524654 -0.578665 -1.008672 -1.248067  0.256005 -0.425916  \n",
       "550 -0.823201 -1.414523 -1.150045 -1.305831 -1.745063 -0.716282 -0.998915  \n",
       "551 -0.780484 -1.037534 -0.483880 -0.555498 -0.768581  0.433960 -0.200928  \n",
       "552 -0.502558 -0.393146 -0.940628 -0.890701 -0.755639 -0.798788 -1.058764  \n",
       "553 -1.027998 -0.967396 -1.089612 -0.922365 -1.354653 -0.753491 -0.555035  \n",
       "554 -0.500800 -0.423831 -0.586935 -0.135715 -0.756400 -0.855411 -0.638713  \n",
       "555 -0.919359  0.264392 -0.529682 -0.346326 -0.355331 -1.091607 -0.061834  \n",
       "556 -0.937465 -0.257255 -0.854113 -1.257616 -1.405205 -1.033367 -0.915792  \n",
       "557 -0.966822 -1.098904 -1.162132 -1.305831 -1.745063 -0.688779 -0.789998  \n",
       "558 -0.258559 -1.304933  0.399718  0.451022 -0.062524 -1.039838 -0.216444  \n",
       "559 -0.714386 -0.112597 -0.016317  0.435670 -0.275239 -1.276034  0.186983  \n",
       "560 -0.305671 -0.362461 -0.177261 -0.669679 -0.149315 -1.052780 -0.040776  \n",
       "561 -0.775210 -1.740223 -1.267986 -1.305831 -1.745063 -2.159342 -1.379622  \n",
       "562  0.060502  0.409050  3.418837  4.307272  1.842324  1.922319  3.156163  \n",
       "563  1.649655  0.365215  1.045400  1.860055  2.125538  0.045693  0.819278  \n",
       "564  2.015301  0.378365 -0.273318  0.664512  1.629151 -1.360158 -0.709091  \n",
       "565  1.494959 -0.691230 -0.394820  0.236573  0.733827 -0.531855 -0.973978  \n",
       "566  0.427906 -0.809587  0.350735  0.326767  0.414069 -1.104549 -0.318409  \n",
       "567  1.653171  1.430427  3.904848  3.197605  2.289985  1.919083  2.219635  \n",
       "568 -1.075813 -1.859019 -1.207552 -1.305831 -1.745063 -0.048138 -0.751207  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "results=BCancer_Data[\"diagnosis\"]\n",
    "BCancer_Data.drop(\"diagnosis\", axis=1, inplace=True)\n",
    "BCancer_Data.drop(\"id\", axis=1, inplace=True)\n",
    "BCancer_Data= StandardScaler().fit_transform(BCancer_Data)\n",
    "BCancer_Data = pd.DataFrame(BCancer_Data)\n",
    "BCancer_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(9)\n",
    "length=len(BCancer_Data)\n",
    "test_size=int(length*0.2)\n",
    "train_size=length-test_size\n",
    "b=list(range(0,length))\n",
    "a=random.sample(b, test_size)\n",
    "a.sort()\n",
    "c=[]\n",
    "for i in b:\n",
    "    if i in a:\n",
    "        continue;\n",
    "    else:\n",
    "        c.append(i)\n",
    "c.sort()\n",
    "del b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=BCancer_Data.loc[c]\n",
    "test=BCancer_Data.loc[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_actual_results=results[a]\n",
    "##test.drop(\"diagnosis\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "train_result=results[c]\n",
    "##train.drop(\"diagnosis\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##train.drop(\"id\", axis=1, inplace=True)\n",
    "##test.drop(\"id\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm   \n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import sklearn.neural_network as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'C': 200000, 'gamma': 1e-06}\n",
      "0.980263157895\n",
      "0.964601769912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:418: ChangedBehaviorWarning: The long-standing behavior to use the estimator's score function in GridSearchCV.score has changed. The scoring parameter is now used.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    }
   ],
   "source": [
    "#### SVM algorithm\n",
    "c=[0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,10,100,1000,10000,100000,100000*2,100000*2.5,100000*3,10000000,1000000000]\n",
    "tuned_parameters = [{'gamma': [1e-3, 1e-4,1e-5,1e-6,1e-7,1e-8,1,10,100,1000,10000,100000,10000000], 'kernel': ['linear','rbf'], 'C': [0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,10,100,1000,10000,100000,100000*2,100000*2.5,100000*3,10000000,1000000000]}]\n",
    "svmal = GridSearchCV(svm.SVC(C=1),tuned_parameters,cv=10,scoring='accuracy')\n",
    "svmal.fit(train, train_result)\n",
    "print(svmal.best_params_)\n",
    "print(svmal.best_score_)\n",
    "print(svmal.score(test,test_actual_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94690265486725667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Random Forest Classifier\n",
    "rf=RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "rf.fit(train,train_result)\n",
    "y_pred_RF=rf.predict(test)\n",
    "rf.score(test, test_actual_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### AdaBoost Classifier\n",
    "ad=AdaBoostClassifier( n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)  \n",
    "###parameters n_estimators,learning rate\n",
    "ad.fit(train,train_result)\n",
    "ad.score(test, test_actual_results)\n",
    "y_pred_ad=ad.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GNB score is: \n",
      "0.938053097345\n"
     ]
    }
   ],
   "source": [
    "#### GaussianNB\n",
    "gb=GaussianNB()\n",
    "gb.fit(train,train_result)\n",
    "y_pred_GNB=gb.predict(test)\n",
    "score=gb.score(test, test_actual_results)\n",
    "print(\"The GNB score is: \")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:418: ChangedBehaviorWarning: The long-standing behavior to use the estimator's score function in GridSearchCV.score has changed. The scoring parameter is now used.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97345132743362828"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Logisitc Regression\n",
    "c=[0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,10,100,1000,10000,100000,100000*2,100000*2.5,100000*3,10000000,1000000000]\n",
    "tuned_parameters = [{'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag'], 'C': [0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,10,100,1000,10000,100000,100000*2,100000*2.5,100000*3,10000000,1000000000]}]\n",
    "logistic = GridSearchCV(linear_model.LogisticRegression(),tuned_parameters,cv=10,scoring='accuracy')\n",
    "logistic.fit(train, train_result)        \n",
    "y_pred_LR=logistic.predict(test)\n",
    "logistic.score(test, test_actual_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.neural_network' has no attribute 'MLPClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f28536d3991b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#### Neural Network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtuned_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'activation'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'identity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'solver'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'invscaling'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'hidden_layer_sizes'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m350\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m450\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'alpha'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.00000001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0000001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.000001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000000000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtuned_parameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_actual_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn.neural_network' has no attribute 'MLPClassifier'"
     ]
    }
   ],
   "source": [
    "#### Neural Network\n",
    "#tuned_parameters = [{'activation' : ['identity', 'logistic', 'tanh', 'relu'] ,'solver':['adam'] , 'learning_rate':'invscaling' ,'hidden_layer_sizes':[100,150,200,250,300,350,400,450,500],'alpha':[0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,10,100,1000,10000,100000,100000*2,100000*2.5,100000*3,10000000,1000000000]}]\n",
    "#mlp=GridSearchCV(nn.MLPClassifier(),tuned_parameters,cv=10,scoring='accuracy')\n",
    "#mlp.fit(train,train_result)\n",
    "#mlp.score(test, test_actual_results)\n",
    "#mlp.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
